{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib notebook\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"numpy_path\")\n",
    "import numpy as np\n",
    "import struct\n",
    "from matplotlib import pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "import keras.callbacks as cb\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from math import cos, sin, pi\n",
    "from statistics import mean\n",
    "import os.path\n",
    "import math\n",
    "shape_size = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss  history\n",
    "class LossHistory(cb.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        batch_loss = logs.get('loss')\n",
    "        self.losses.append(batch_loss)\n",
    "\n",
    "#plot losses\n",
    "def plot_losses(losses):\n",
    "    plt.plot(losses)\n",
    "    plt.title('Loss per batch')\n",
    "    plt.show()\n",
    "    \n",
    "def feature_scaling(X):\n",
    "    X = X.T\n",
    "    for i in range(7):\n",
    "        mean = X[i].mean()\n",
    "        std = X[i].std()\n",
    "        X[i] = [(x - mean)/std for x in X[i]]\n",
    "    return X.T\n",
    "\n",
    "# input dimension\n",
    "in_dim = 6\n",
    "out_dim = 200\n",
    "def init_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=in_dim))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(500))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(500))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(200))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(200))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(Dense(70))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Dense(100))\n",
    "#     model.add(Activation('relu'))\n",
    "    model.add(Dense(out_dim))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    # use mean squared error to measure the looses\n",
    "    model.compile(loss=keras.losses.mean_squared_error,\n",
    "              optimizer=keras.optimizers.Adam(lr = 0.001),\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2 has \t7\n",
      "Batch 3 has \t6\n",
      "Batch 4 has \t17\n",
      "Batch 5 has \t5\n",
      "Batch 6 has \t14\n",
      "Batch 7 has \t14\n",
      "Batch 8 has \t40\n",
      "Batch 9 has \t5\n",
      "Batch 10 has \t14\n",
      "Batch 11 has \t14\n",
      "Batch 12 has \t40\n",
      "Batch 13 has \t13\n",
      "Batch 14 has \t37\n",
      "Batch 15 has \t37\n",
      "Batch 16 has \t106\n",
      "Batch 17 has \t5\n",
      "Batch 18 has \t14\n",
      "Batch 19 has \t14\n",
      "Batch 20 has \t40\n",
      "Batch 21 has \t13\n",
      "Batch 22 has \t37\n",
      "Batch 23 has \t37\n",
      "Batch 24 has \t106\n",
      "Batch 25 has \t13\n",
      "Batch 26 has \t37\n",
      "Batch 27 has \t37\n",
      "Batch 28 has \t106\n",
      "Batch 29 has \t35\n",
      "Batch 30 has \t100\n",
      "Batch 31 has \t100\n",
      "Batch 32 has \t287\n",
      "Batch 33 has \t5\n",
      "Batch 34 has \t14\n",
      "Batch 35 has \t14\n",
      "Batch 36 has \t40\n",
      "Batch 37 has \t13\n",
      "Batch 38 has \t37\n",
      "Batch 39 has \t37\n",
      "Batch 40 has \t106\n",
      "Batch 41 has \t13\n",
      "Batch 42 has \t37\n",
      "Batch 43 has \t37\n",
      "Batch 44 has \t106\n",
      "Batch 45 has \t35\n",
      "Batch 46 has \t100\n",
      "Batch 47 has \t100\n",
      "Batch 48 has \t287\n",
      "Batch 49 has \t13\n",
      "Batch 50 has \t37\n",
      "Batch 51 has \t37\n",
      "Batch 52 has \t106\n",
      "Batch 53 has \t35\n",
      "Batch 54 has \t100\n",
      "Batch 55 has \t100\n",
      "Batch 56 has \t287\n",
      "Batch 57 has \t35\n",
      "Batch 58 has \t100\n",
      "Batch 59 has \t100\n",
      "Batch 60 has \t287\n",
      "Batch 61 has \t95\n",
      "Batch 62 has \t272\n",
      "Batch 63 has \t272\n",
      "Batch 64 has \t781\n",
      "Total # of data: 4989\n"
     ]
    }
   ],
   "source": [
    "data_size = 0\n",
    "dummy1 = [0]*200\n",
    "dummy2 = [0]*6\n",
    "SP = np.array(np.reshape(dummy1, (1, 200)))\n",
    "SH = np.array(np.reshape(dummy2, (1, 6)))\n",
    "for i in range(2, 65):\n",
    "    path = 'meep_code/data/DATA'+str(i)\n",
    "    if not os.path.exists(path):\n",
    "        #miss.append(i)\n",
    "        print('Missing batch:' + str(i))\n",
    "        continue\n",
    "        \n",
    "    files = next(os.walk(path))[2] #dir is your directory path as string]\n",
    "    num_data = len(files)\n",
    "    data_size += num_data\n",
    "    skip = []\n",
    "    \n",
    "    coordinates = np.genfromtxt('meep_code/data/DATA'+str(i)+'_sh.txt')\n",
    "    xc, yc = coordinates[:, 0], coordinates[:, 1]\n",
    "    xc = np.reshape(xc, (num_data, shape_size))\n",
    "    yc = np.reshape(yc, (num_data, shape_size))\n",
    "    \n",
    "    for j in range(num_data):\n",
    "        tmp = np.genfromtxt(path+'/'+'DATA'+str(i)+'_sp'+str(j)+'.txt')\n",
    "        valid = True\n",
    "        for q in range(200):\n",
    "            if math.isnan(float(tmp[q])):\n",
    "                print('Batch '+str(i)+'\\tsample '+str(j)+' has NAN value')\n",
    "                valid = False\n",
    "                break\n",
    "            if tmp[q] > 3:\n",
    "                print('Batch '+str(i)+'\\tsample '+str(j)+' has extreme value')\n",
    "                valid = False\n",
    "                break\n",
    "        if not valid:\n",
    "            #skip.append(j)\n",
    "            continue\n",
    "        SP = np.concatenate((SP, np.reshape(tmp, (1, 200))))\n",
    "        tmp = []\n",
    "        for q in range(6):\n",
    "            tmp.append(math.sqrt(xc[j][q]**2 + yc[j][q]**2))\n",
    "        SH = np.concatenate((SH, np.reshape(np.array(tmp), (1, 6))))\n",
    "    print('Batch '+str(i)+' has \\t'+str(num_data)) \n",
    "print('Total # of data: ' + str(len(SH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4989"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "SP_F, SH_F = np.reshape(SP[1], (1, 200)),np.reshape(SH[1], (1, 6))\n",
    "for i in range(2, len(SP)):\n",
    "    peak = 0\n",
    "    for j in range(1, 200):\n",
    "        if SP[i][j - 1] >= 0.6 >=SP[i][j]:\n",
    "            peak += 1\n",
    "            #p_pos = [((j-1)/2+200,SP[i][j - 1]), (j/2+200, SP[i][j])]\n",
    "        if peak < 5:\n",
    "            SP_F = np.concatenate((SP_F, np.reshape(SP[i], (1, 200))))\n",
    "            SH_F = np.concatenate((SH_F, np.reshape(SH[i], (1, 6))))\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = np.append(SP_F, SH_F, axis = 1)\n",
    "np.random.shuffle(DATA)\n",
    "\n",
    "Y = DATA[:, :200]\n",
    "X = DATA[:,200:]\n",
    "\n",
    "train_size = int(len(DATA) * 0.8)\n",
    "\n",
    "train_X = X[0:train_size, :]\n",
    "train_Y = Y[0:train_size, :]\n",
    "test_X = X[train_size:, :]\n",
    "test_Y = Y[train_size:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_model()\n",
    "#history = LossHistory()\n",
    "# when training, using minibatch seems to be pretty good\n",
    "history = model.fit(train_X, train_Y, epochs=1000, batch_size=20,\n",
    "                callbacks=[history],\n",
    "                validation_data=(test_X, test_Y), verbose=2)\n",
    "# score = model.evaluate(test_X, test_Y, batch_size=10)\n",
    "# print(score)\n",
    "# plot_losses(history.losses)\n",
    "\n",
    "# callbacks = [EarlyStopping(monitor='val_loss', patience=100),\n",
    "#              ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "# model = init_model_2()\n",
    "# history = model.fit(train_x, train_y_processed, \n",
    "#                     epochs=1000,\n",
    "#                     callbacks=callbacks, \n",
    "#                     batch_size=400, \n",
    "#                     validation_data=(test_x, test_y_processed), verbose = 2)\n",
    "train_score = model.evaluate(train_X, train_Y, batch_size=100)\n",
    "test_score = model.evaluate(test_x, test_Y, batch_size= 50)\n",
    "print(train_score)\n",
    "print(test_score)\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
